{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'medical-science': 2,\n",
       " 'nutrition': 0,\n",
       " 'psychology': 4,\n",
       " 'climate-change': 1,\n",
       " 'physics': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"dataset/web_science_dataset.jsonl\"\n",
    "json_data = []\n",
    "with open(filename) as f:\n",
    "    json_data = f.readlines()\n",
    "json_data_list = []\n",
    "for item in json_data:\n",
    "    json_data_list.append(json.loads(item))\n",
    "df = pd.DataFrame(json_data_list)\n",
    "\n",
    "#labels\n",
    "label_map = dict(zip(df['category'],df['categoryId']))\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return (' '.join(lemmatizer.lemmatize(w) for w in word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):    \n",
    "\n",
    "    # remove leading/trailing spaces\n",
    "    df = df.str.strip()\n",
    "    \n",
    "    # convert to lowercase\n",
    "    df = df.str.lower()\n",
    "    \n",
    "    #df = df.replace(to_replace ='http\\S+', value = '', regex = True)\n",
    "    \n",
    "    # remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    df = df.str.translate(translator)\n",
    "    \n",
    "    # remove non-alphanumeric characters\n",
    "    df = df.replace(to_replace ='\\s*[^A-Za-z0-9]+\\s*', value = ' ', regex = True)\n",
    "    \n",
    "    # remove digits\n",
    "    translator = str.maketrans('', '', string.digits) \n",
    "    df = df.str.translate(translator)\n",
    "    \n",
    "    df = df.str.strip()\n",
    "    \n",
    "    #lemmatize\n",
    "    df = df.apply(lemmatize_text)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>questionId</th>\n",
       "      <th>questionUrl</th>\n",
       "      <th>category</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>answer</th>\n",
       "      <th>answerUrl</th>\n",
       "      <th>answerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>can headbanging cause brain damage</td>\n",
       "      <td>14138</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>medical-science</td>\n",
       "      <td>2</td>\n",
       "      <td>A number of injuries have been attributed to t...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>14139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>doe the shangrila diet work according to it su...</td>\n",
       "      <td>10103</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>0</td>\n",
       "      <td>The Shangri-La diet depends on two theories:\\n...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>16121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>can phobia be genetic but created in one gener...</td>\n",
       "      <td>18713</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>psychology</td>\n",
       "      <td>4</td>\n",
       "      <td>This question has remained unanswered yet not ...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>22322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>do of u american think that global warming is ...</td>\n",
       "      <td>36010</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/3...</td>\n",
       "      <td>climate-change</td>\n",
       "      <td>1</td>\n",
       "      <td>The&amp;nbsp;40% figure most likely comes from Pew...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/3...</td>\n",
       "      <td>36011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>doe boiling the same water twice make it dange...</td>\n",
       "      <td>11118</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>0</td>\n",
       "      <td>The claims\\n\\n\\nevery time the same water is b...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/1...</td>\n",
       "      <td>11119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question questionId  \\\n",
       "0                 can headbanging cause brain damage      14138   \n",
       "1  doe the shangrila diet work according to it su...      10103   \n",
       "2  can phobia be genetic but created in one gener...      18713   \n",
       "3  do of u american think that global warming is ...      36010   \n",
       "4  doe boiling the same water twice make it dange...      11118   \n",
       "\n",
       "                                         questionUrl         category  \\\n",
       "0  https://skeptics.stackexchange.com/questions/1...  medical-science   \n",
       "1  https://skeptics.stackexchange.com/questions/1...        nutrition   \n",
       "2  https://skeptics.stackexchange.com/questions/1...       psychology   \n",
       "3  https://skeptics.stackexchange.com/questions/3...   climate-change   \n",
       "4  https://skeptics.stackexchange.com/questions/1...        nutrition   \n",
       "\n",
       "   categoryId                                             answer  \\\n",
       "0           2  A number of injuries have been attributed to t...   \n",
       "1           0  The Shangri-La diet depends on two theories:\\n...   \n",
       "2           4  This question has remained unanswered yet not ...   \n",
       "3           1  The&nbsp;40% figure most likely comes from Pew...   \n",
       "4           0  The claims\\n\\n\\nevery time the same water is b...   \n",
       "\n",
       "                                           answerUrl answerId  \n",
       "0  https://skeptics.stackexchange.com/questions/1...    14139  \n",
       "1  https://skeptics.stackexchange.com/questions/1...    16121  \n",
       "2  https://skeptics.stackexchange.com/questions/1...    22322  \n",
       "3  https://skeptics.stackexchange.com/questions/3...    36011  \n",
       "4  https://skeptics.stackexchange.com/questions/1...    11119  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['question'] = preprocess_text(df['question'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        df, df['category'],stratify=df['category'], test_size=0.4)\n",
    "\n",
    "test_data, valid_data, test_labels, valid_labels = train_test_split(\n",
    "        test_data, test_labels, stratify=test_labels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>questionId</th>\n",
       "      <th>questionUrl</th>\n",
       "      <th>category</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>answer</th>\n",
       "      <th>answerUrl</th>\n",
       "      <th>answerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>doe garlic enhance immune system</td>\n",
       "      <td>20713</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2...</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>0</td>\n",
       "      <td>I'll take it that having potent anti-cancer pr...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2...</td>\n",
       "      <td>20727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>doe fda not test anything in order to approve it</td>\n",
       "      <td>4309</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/4309</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>0</td>\n",
       "      <td>The first two parts are easy to address. From ...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/4...</td>\n",
       "      <td>4312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>is coconut oil the best for hightemperature co...</td>\n",
       "      <td>23317</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2...</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>0</td>\n",
       "      <td>Coconut Oil does suffer from this behaviour, a...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2...</td>\n",
       "      <td>23362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>is it safe to stand by the window during a thu...</td>\n",
       "      <td>22562</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2...</td>\n",
       "      <td>physics</td>\n",
       "      <td>3</td>\n",
       "      <td>It's not just your culture.  Advice from the U...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2...</td>\n",
       "      <td>22565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>is eating clovenhoofed animal bottomfeeders or...</td>\n",
       "      <td>2330</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2330</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>0</td>\n",
       "      <td>Judiasm 101 has some interesting insights into...</td>\n",
       "      <td>https://skeptics.stackexchange.com/questions/2...</td>\n",
       "      <td>2347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question questionId  \\\n",
       "68                    doe garlic enhance immune system      20713   \n",
       "548   doe fda not test anything in order to approve it       4309   \n",
       "645  is coconut oil the best for hightemperature co...      23317   \n",
       "753  is it safe to stand by the window during a thu...      22562   \n",
       "13   is eating clovenhoofed animal bottomfeeders or...       2330   \n",
       "\n",
       "                                           questionUrl   category  categoryId  \\\n",
       "68   https://skeptics.stackexchange.com/questions/2...  nutrition           0   \n",
       "548  https://skeptics.stackexchange.com/questions/4309  nutrition           0   \n",
       "645  https://skeptics.stackexchange.com/questions/2...  nutrition           0   \n",
       "753  https://skeptics.stackexchange.com/questions/2...    physics           3   \n",
       "13   https://skeptics.stackexchange.com/questions/2330  nutrition           0   \n",
       "\n",
       "                                                answer  \\\n",
       "68   I'll take it that having potent anti-cancer pr...   \n",
       "548  The first two parts are easy to address. From ...   \n",
       "645  Coconut Oil does suffer from this behaviour, a...   \n",
       "753  It's not just your culture.  Advice from the U...   \n",
       "13   Judiasm 101 has some interesting insights into...   \n",
       "\n",
       "                                             answerUrl answerId  \n",
       "68   https://skeptics.stackexchange.com/questions/2...    20727  \n",
       "548  https://skeptics.stackexchange.com/questions/4...     4312  \n",
       "645  https://skeptics.stackexchange.com/questions/2...    23362  \n",
       "753  https://skeptics.stackexchange.com/questions/2...    22565  \n",
       "13   https://skeptics.stackexchange.com/questions/2...     2347  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained word embeddings: WordToVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"C:\\\\Users\\\\Reen\\\\Desktop\\\\web science\\\\WordEmbeddings\\\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "word_to_vec_model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True,limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = np.concatenate([word_to_vec_model.vectors, np.zeros(shape=(1,300))], axis=0) #last vector padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of code provided in the tutorial was used. Reference: liar_liar_bilstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "num_labels = len(label_map)\n",
    "batch_size = 64\n",
    "lr = 2e-4\n",
    "lstm_dim = 200\n",
    "n_epochs = 20\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDatasetReader():\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.values[idx]\n",
    "        input_seq,seq_lens = text_to_batch_bilstm([row[0]]) #sending a list of one row\n",
    "        label = label_map[row[3]]\n",
    "        return input_seq, seq_lens, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    input_ids = [i[0][0] for i in input_data]\n",
    "    seq_lens = [i[1][0] for i in input_data]\n",
    "    labels = [i[2] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
    "\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_batch_bilstm(text: List) -> Tuple[List, List]:\n",
    "    oov_word_token = 500000 #oov token\n",
    "    \n",
    "    sentences = []\n",
    "    for t in text:\n",
    "        t = t.lower()\n",
    "        t = ''.join([c for c in t if c not in punctuation])\n",
    "    sentences.append(t)\n",
    "        \n",
    "    tokens = [ list(tokenize(t)) for t in sentences]\n",
    "    #print(tokens)\n",
    "    \n",
    "    input_ids = [[word_to_vec_model.vocab[token].index for token in sentence if token in word_to_vec_model] for sentence in tokens]\n",
    "    #print(input_ids)\n",
    "    \n",
    "    #input_ids = [tokenizer.encode_ids_with_eos(t) for t in text]\n",
    "\n",
    "    return input_ids, [len(ids) for ids in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset readers and read data\n",
    "train_dataset = ClassificationDatasetReader(train_data)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm)\n",
    "\n",
    "valid_dataset = ClassificationDatasetReader(valid_data)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=len(valid_data), collate_fn=collate_batch_bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class LSTMNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int, dropout_prob: float = 0.1, n_classes: int = 5):\n",
    "        \n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        \n",
    "        #LSTM input\n",
    "        self.input_size = pretrained_embeddings.shape[1] #features in input\n",
    "        self.hidden_size = 200 \n",
    "        self.num_layers = 1 #default \n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1)\n",
    "        self.lstm = nn.LSTM(self.input_size,self.hidden_size,self.num_layers,batch_first=True,dropout=dropout_prob,bidirectional=True)\n",
    "        self.ff = nn.Linear(2*self.hidden_size,n_classes)\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        \n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.embeddings(inputs)\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        \n",
    "        # Get the last output for classification (b x 2*lstm_dim)\n",
    "        ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
    "        #print(input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)))\n",
    "        \n",
    "        # Get logits (b x 2)\n",
    "        logits = self.ff(ff_in).view(-1, self.n_classes)\n",
    "        \n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Xentropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = LSTMNetwork(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings), \n",
    "    lstm_dim=lstm_dim, \n",
    "    dropout_prob=0, \n",
    "    n_classes=len(label_map)\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    return torch.sum(torch.argmax(logits, dim=-1) == labels).type(torch.float) / float(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, optimizer, n_epochs, device):\n",
    "    \n",
    "    losses = []\n",
    "    best_acc = 0.0\n",
    "  \n",
    "    for ep in range(n_epochs):\n",
    "        loss_epoch = []\n",
    "        \n",
    "        for batch in tqdm(train_dl):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids = batch[0]\n",
    "            seq_lens = batch[1]\n",
    "            labels = batch[2]\n",
    "\n",
    "            loss, logits = model(input_ids, seq_lens, labels=labels)\n",
    "            losses.append(loss.item())\n",
    "            loss_epoch.append(loss.item())\n",
    "      \n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "        #gc.collect()\n",
    "    \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dl:\n",
    "\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids = batch[0]\n",
    "                seq_lens = batch[1]\n",
    "                labels = batch[2]\n",
    "\n",
    "                val_loss, logits = model(input_ids, seq_lens, labels=labels)\n",
    "                acc = accuracy(logits, labels)\n",
    "\n",
    "                print(f'Validation accuracy: {acc}, train loss: {sum(loss_epoch) / len(loss_epoch)}',\"val_loss: \",val_loss.item())\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "                if acc > best_acc:\n",
    "                  #best_model = model.state_dict()\n",
    "                    best_acc = acc\n",
    "                #gc.collect()\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d330bc126dd48beac406e9b69f666b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.37383174896240234, train loss: 1.5731640815734864 val_loss:  1.530668020248413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335e652070cc4299bb5921b31aa11f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.37383174896240234, train loss: 1.5062084436416625 val_loss:  1.4802632331848145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d5cd2fb5b545cf92f58330824a92b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.37383174896240234, train loss: 1.477591335773468 val_loss:  1.4621939659118652\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92326b44a9e2410697ccabf5ce1decc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.37383174896240234, train loss: 1.4493348956108094 val_loss:  1.4399510622024536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c46ea7b28534ff19a74faa6ff64dc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.37383174896240234, train loss: 1.4258828282356262 val_loss:  1.4084078073501587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeb724dd08a4746961a07b1e2c31d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.37383174896240234, train loss: 1.3812713146209716 val_loss:  1.34652578830719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13906f0973d7402ebb804cab60137ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.40186914801597595, train loss: 1.3012239575386046 val_loss:  1.243157982826233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d432f31372214db18ab32cfb538e25e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.5327102541923523, train loss: 1.215136206150055 val_loss:  1.1528927087783813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7c2efaad234f3e9ab4fc843222f01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.5747663378715515, train loss: 1.1518130421638488 val_loss:  1.0899977684020996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c03f26c773454f8456b7360fbab480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.5700934529304504, train loss: 1.1002074837684632 val_loss:  1.065401315689087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811143918ba246cba92dbf0de3b93cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.5560747385025024, train loss: 1.0564054250717163 val_loss:  1.0314016342163086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fac6f3ad96495d80e3a76982e12396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.630841076374054, train loss: 1.0074251472949982 val_loss:  1.008083462715149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04851474f5ee4bdb9f62fd6b8bac795e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.6355140209197998, train loss: 0.9604259192943573 val_loss:  0.954371988773346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2092e74487a842a6b06c85836f0f316d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.663551390171051, train loss: 0.8983135044574737 val_loss:  0.896719217300415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f7184e597449cf961cbc0f99a4b9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.7102803587913513, train loss: 0.8419132351875305 val_loss:  0.8528652787208557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fa2dfe377947f992fcfb61dd224cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.7242990136146545, train loss: 0.7886219799518586 val_loss:  0.7867127656936646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2465fd585c0c4eb69bb8fbb91d1a31e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.7196261286735535, train loss: 0.7240154325962067 val_loss:  0.7450224757194519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ddca71c5fc4fa1958397bd2ee12cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.7336448431015015, train loss: 0.6837927997112274 val_loss:  0.7154514789581299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebb514538cc467da8b7e2e03ff2389e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.7757009267807007, train loss: 0.6431334972381592 val_loss:  0.7034911513328552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3904a1d553104b05b3bb34a8f634bf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.7710280418395996, train loss: 0.6063642382621766 val_loss:  0.6997005343437195\n"
     ]
    }
   ],
   "source": [
    "# Create the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
